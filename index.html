
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Personal Page of Tao He</title>
<script>-.
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?5ba3d71ef2aa53ddd7c46bcc9e30c309";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

<style type="text/css">
<!--
body,td,th {
	font-family: Georgia, Times New Roman, Times, serif;
}
li {line-height:25px}
-->
</style></head>

<body style="margin:100px 200px">


		<div class="block">
		
		<div id="main">
			<div style="width:130px; float:right; margin: 70px 70px 0 0;">
				<img src="hetao.jpg" width="200px">
			</div>
			<div class="block name nodec">
				<h2 class="name">Tao He  </h2>
				<h2 class="name chinese">何涛</h2>
			</div>
			<div class="block">
				<div>
					<ul class="contact-list">
						<li id="contact-address">
							<span class="label">Address:</span>
							<span>
								Machine Intelligence Laboratory<br>
								College of Computer Science<br>
								Sichuan University, Chengdu 610065, P. R. China
							</span>
						</li>
						
					    <li>
							<span class="label">Email:</span>
							<span>taohe@stu.scu.edu.cn <strong>or</strong> ithet1007@163.com</span>
						</li>
					</ul>
				</div>
			</div>								
			<div class="block">
				<span>Last Updated: May 26, 2016</span>
			</div>
		</div>
		
		
		
		
<h2><strong>Self-introduction</strong></h2><hr/>
<p>Now I am a Master student supervised by <a href="http://www.machineilab.org/users/zhangyi/index.htm" target="_blank">Prof. Zhang Yi</a> in<a href="http://www.machineilab.org" target="_blank"> machine intelligence lab</a>, <a href="http://www.scu.edu.cn" target="_blank">Sichuan University</a>, China. My research area concernes in reinforcement learning and neural networks, especially deep learning and recurrent neural networks. Meanwhile, I focus on visual analysis and audio processing. In particular, I try my test to analyse the dynamical foundation of recurrent neural networks. I am interesting in finding the mathematical theoretical basis of deep learning. 
I am also interested in dealing with the practical problems by deep learning techniques. The applications that I am participating in include speech recognition (particularly Sichuan Dialect), speech emotion analysis, speech synthesis, computer vision, etc. I am planning to pay more interests on speech analysis for further research in the future research.
</p>
</p>
<p/><p/>
<div style="float:none">&nbsp;</div>
<h2><strong>Journal</strong></h2>
<hr/>
<li><em><strong>Tao He</em></strong>, Hua Mao, and Zhang Yi. Moving object recognition using multi-view three-dimensional convolutional neural networks. Neural Computing and Applications, pages 1–9, 2016. [<a href="http://link.springer.com/article/10.1007%2Fs00521-016-2277-9" target="_blank"> pdf</a>]</li>
<p/><em><strong>Abstract:</em></strong>Moving object recognition (MOR) is an important but challenging problem in the field of computer vision. The aim of MOR is to recognize moving objects in a given video dataset. Convolutional neural networks (CNNs) have been extensively used for image recognition and video analysis problems. Recently, a 3D-CNN, which contains 3D convolution layers, was proposed to address MOR problems by successfully extracting spatiotemporal features. In this paper, a multi-view (MV) 3D-CNN is proposed for MOR. This model combines 3D-CNNs with a well-known MV learning technique. Because multi-view learning techniques have the ability to obtain more view-related features from videos captured by different cameras, the proposed model can extract more representative features. Moreover, the model contains a special view-pooling layer that can fuse the feature information from previous layers. The proposed MV3D-CNN is applied to both real-world moving vehicle recognition and sign language recognition tasks. The experimental results show that the proposed model possesses good performance.<p/>
<p/><p/>
<p/><p/>
<li><em><strong>Tao He</em></strong>,  Hua Mao, Jixiang Guo, and
Zhang Yi. Cell tracking using deep neural networks
with multi-task learning. Image and Vision Computing, 2016. [<a href="http://www.sciencedirect.com/science/article/pii/S0262885616302001" target="_blank"> pdf</a>][<a href="./cell_data/all_datas.rar"> dataset</a>][<a href="https://github.com/ithet1007/Cell-Tracking">code</a>]</li>
<p/><em><strong>Abstract:</em></strong>Abstract Cell tracking plays crucial role in biomedical and computer vision areas. As cells generally have frequent deformation activities and small sizes in microscope image, tracking the non-rigid and non-significant cells is quite difficult in practice. Traditional visual tracking methods have good performances on tracking rigid and significant visual objects, however, they are not suitable for cell tracking problem. In this paper, a novel cell tracking method is proposed by using Convolutional Neural Networks (CNNs) as well as Multi-Task Learning (MTL) techniques. The CNNs learn robust cell features and MTL improves the generalization performance of the tracking. The proposed cell tracking method consists of a particle filter motion model, a multi-task learning observation model, and an optimized model update strategy. In the training procedure, the cell tracking is divided into an online tracking task and an accompanying classification task using the MTL technique. The observation model is trained by building a CNN to learn robust cell features. The tracking procedure is started by assigning the cell position in the first frame of a microscope image sequence. Then, the particle filter model is applied to produce a set of candidate bounding boxes in the subsequent frames. The trained observation model provides the confidence probabilities corresponding to all of the candidates and selects the candidate with the highest probability as the final prediction. Finally, an optimized model update strategy is proposed to enable the multi-task observation model for the variation of the tracked cell over the entire tracking procedure. The performance and robustness of the proposed method is analyzed by comparing with other commonly-used methods. Experimental results demonstrate that the proposed method has good performance to the cell tracking problem.<p/>
<p/><p/>
<p/><p/>
<div style="float:none">&nbsp;</div>
<h2><strong>Conference</strong></h2>
<hr/>
<li>Jia Jia, Jie Huang, Guangyao Shen, <em><strong>Tao He</em></strong>, Zhiyuan Liu, Huanbo Luan, Chao Yan. Learning to Appreciate the Aesthetic Effects of Clothing. In Proceedings
of the 30th Association for the Advancement of Artificial
Intelligence (AAAI), pages 1216–1222, 2016.[<a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/viewPDFInterstitial/12056/11725" target="_blank"> pdf</a>]</li>
<li>Boya Wu, Jia Jia, <em><strong>Tao He</em></strong>, Juan Du, Xiaoyuan Yi, Yishuang Ning.  Inferring users' emotions for human-mobile voice dialogue applications. In Pro-
ceedings of the International Conference on Multimedia
and Expo (ICME), pages 1–6, 2016.[<a href="http://hcsi.cs.tsinghua.edu.cn/static/Paper/Paper16/BoyaWu.pdf" target="_blank"> pdf</a>]</li>
<p/><p/>

<h2><strong>Academic Services</strong></h2>
<hr/>
<li>Student Member of IEEE and IEEE Computational Intelligence Society (CIS).</li>
<h2><strong>Education</strong></h2>
<hr/>
<li>Sep,2014--present. Master Candidate, College of Computer Science, Sichuan University, Chengdu.</li>
<li>Sep,2010--Jul,2014. Undergraduate, Dept. of Software Engineering, Sichuan University, Chengdu.</li>
<p/><p/>

<h2><strong>Experience</strong></h2>
<hr/>
<li>May,2015--Oct,2015. I went to Tsinghua University as an academic visitor. I did some affective computing research at Human Computer Speech Interaction Research Group for about five monthes.</li>
<p/><p/>

<h2><strong>Awards</strong></h2>
<hr/>
<li>Dec.2015. Excellent Contribution Award. Machine Intelligence Laboratory.</li>
<p/></p>

</body>
</html>
